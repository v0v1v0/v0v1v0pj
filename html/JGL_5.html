<div class="container">

<table style="width: 100%;"><tr>
<td>JGL</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Joint Graphical Lasso
</h2>

<h3>Description</h3>

<p>Solve the Joint Graphical Lasso
</p>


<h3>Usage</h3>

<pre><code class="language-R">JGL(Y,penalty="fused",lambda1,lambda2,rho=1,weights="equal",penalize.diagonal=FALSE,
maxiter=500,tol=1e-5,warm=NULL,return.whole.theta=FALSE, screening="fast",
truncate = 1e-5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>

<p>A list of nXp data matrices.   
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>

<p>Determines whether lambda2 controls a "fused" or "group" lasso penalty.  Must take value "fused" or "group".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda1</code></td>
<td>

<p>The tuning parameter for the graphical lasso penalty.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2</code></td>
<td>

<p>The tuning parameter for the fused or group lasso penalty.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>

<p>A step size parameter.  Large values decrease step size.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>

<p>Determines the putative sample size of each class's data.  Allowed values: a vector with 
length equal to the number of classes; "equal", giving each class weight 1; "sample.size",
giving each class weight corresponding to its sample size.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalize.diagonal</code></td>
<td>

<p>If penalty=="fused", determines whether lambda1 is applied to the diagonal of theta.
If penalty=="group", determines whether lambda1 and lambda2 are applied to the diagonal of theta.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>

<p>Maximum number of iterations.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>

<p>Determines convergence criterion.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warm</code></td>
<td>

<p>Input a warm start to theta in the form of a K-length list of pXp matrices.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return.whole.theta</code></td>
<td>

<p>If TRUE, each class's inverse covariance matrix is returned whole.  If FALSE, the inverse covariance
matrix is only returned over the connected nodes, and only the diagonal of the matrix is returned over
the unconnected nodes.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>screening</code></td>
<td>

<p>"fast" or "memory.efficient".  Use of "fast" is recommended unless the number of features prohibits
storage of a pXp matrix.  For very high dimension data, screening="memory.efficient" will allow a 
solution with a much longer computation time.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>truncate</code></td>
<td>

<p>Defaults to 1e-5.  At convergence, all values of theta below this number will be set to zero.  
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function can solve both the Fused Graphical Lasso and the Group Graphical Lasso.  
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>theta</code></td>
<td>
<p>A list of the estimated inverse covariance matrices, over all nodes if 
return.whole.theta==TRUE and over only the connected nodes if return.whole.theta==FALSE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>diag.theta.unconnected</code></td>
<td>
<p>Returned only if return.whole.theta==FALSE.  A list of vectors, 
each vector the estimated diagonal of an inverse covariance matrix over the unconnected nodes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>connected</code></td>
<td>
<p>A logical vector identifying whether each node is connected.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Patrick Danaher
</p>


<h3>References</h3>

<p>Patrick Danaher, Pei Wang and Daniela Witten (2011).  The joint graphical lasso for inverse covariance 
estimation across multiple classes.  http://arxiv.org/abs/1111.0324
</p>


<h3>Examples</h3>

<pre><code class="language-R">## load an example dataset with K=two classes, p=200 features, and n=100 samples per class:
data(example.data)
str(example.data)
## run fgl:
fgl.results = JGL(Y=example.data,penalty="fused",lambda1=.25,lambda2=.1)
str(fgl.results)
print.jgl(fgl.results)
## run ggl:
ggl.results = JGL(Y=example.data,penalty="group",lambda1=.15,lambda2=.2,return.whole.theta=TRUE)
str(ggl.results)
print.jgl(ggl.results)
</code></pre>


</div>